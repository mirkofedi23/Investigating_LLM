{"cells":[{"cell_type":"markdown","source":["# Mistral 7B Instruct Fine-tuning Script\n","### Contents:\n","- Loading Model from huggingface\n","- Loading Dataset (Manually labelled dataset) from huggingface\n","- Pre-process the training dataset with the Mistral formatting template\n","- Define LoRA model arguments and SFT Trainer\n","- Train the model , recording using Wand B\n","- Referenced Code from\n","  - https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb\n","\n","\n","\n"],"metadata":{"id":"c4snMEYkbWUP"},"id":"c4snMEYkbWUP"},{"cell_type":"markdown","source":["#### Installing Dependencies"],"metadata":{"id":"5i3t1OKRb3bC"},"id":"5i3t1OKRb3bC"},{"cell_type":"code","execution_count":null,"id":"75aa92c7-fff7-4711-b7fe-1110995fb039","metadata":{"collapsed":true,"id":"75aa92c7-fff7-4711-b7fe-1110995fb039","outputId":"800be8ea-20f5-4765-a73b-272033057399"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install -q transformers[torch] datasets\n","!pip install -q bitsandbytes trl peft\n","!pip install flash-attn --no-build-isolation\n","!pip install -q torch\n","!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n","!pip install -q datasets #huggingface datasets to download and manipulate datasets\n","!pip install -q peft #Parameter efficient finetuning - for qLora Finetuning\n","!pip install -q bitsandbytes #For Model weights quantisation\n","!pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n","!pip install -q wandb -U #Used to monitor the model score during training\n","!pip install --upgrade huggingface_hub\n","!pip install wandb\n","!huggingface-cli login --token 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\n","\n","import json\n","import pandas as pd\n","import torch\n","import wandb\n","from datasets import Dataset, load_dataset\n","from huggingface_hub import notebook_login\n","from peft import LoraConfig, PeftModel\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from trl import SFTTrainer"]},{"cell_type":"markdown","source":["### Logging into Huggingface and WandB"],"metadata":{"id":"MfOBGHiacKTj"},"id":"MfOBGHiacKTj"},{"cell_type":"code","execution_count":null,"id":"96820a17-2914-4c13-a128-24c5b9872f2b","metadata":{"colab":{"referenced_widgets":["5fe408a292a74faf80e0844053e01559"]},"collapsed":true,"id":"96820a17-2914-4c13-a128-24c5b9872f2b","outputId":"8af35da6-7ffe-47e8-f7a0-309187bf03e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.4)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.27.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5fe408a292a74faf80e0844053e01559","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["hf_token = #\n","wb_token = #\n","wandb.login(key=wb_token)\n","notebook_login()"]},{"cell_type":"markdown","source":["### Loading Manually labelled dataset for Instruct Fine-Tuning"],"metadata":{"id":"nrNWAOtYcW0J"},"id":"nrNWAOtYcW0J"},{"cell_type":"code","execution_count":null,"id":"ecd252ca-77b8-47f9-bc5b-9d0516a1df7d","metadata":{"colab":{"referenced_widgets":["e9b471eb0ced4927a1028cc3f0241695","e9f310d8c1d7432db78cb61f60299839","9d3e736c917543d5b88b447e22bcee8d"]},"collapsed":true,"id":"ecd252ca-77b8-47f9-bc5b-9d0516a1df7d","outputId":"6972fc3b-2a58-4bf4-9050-2a30b7713726"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9b471eb0ced4927a1028cc3f0241695","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/4.42k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9f310d8c1d7432db78cb61f60299839","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/108k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d3e736c917543d5b88b447e22bcee8d","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/49 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"mirko5301/crypto_whitepaper_public\", split=\"test\")"]},{"cell_type":"code","execution_count":null,"id":"bf826db5-a907-45a8-b878-9cf17b7d2771","metadata":{"id":"bf826db5-a907-45a8-b878-9cf17b7d2771"},"outputs":[],"source":["# Function to format each row in the dataset\n","def create_text_row(instruction, output, input):\n","    text_row = f\"\"\"<s>[INST] {instruction} here are the inputs {input} [/INST] \\\\n {output} </s>\"\"\"\n","    return text_row\n","\n","# Iterate over all the rows, format the dataset, and store it in a JSONL file\n","def process_jsonl_file(dataset, output_file_path):\n","    with open(output_file_path, \"w\") as output_jsonl_file:\n","        for item in dataset:\n","            json_object = {\n","                \"text\": create_text_row(item[\"instruction\"], item[\"output\"], item[\"input\"]),\n","                \"instruction\": item[\"instruction\"],\n","                \"input\": item[\"input\"],\n","                \"output\": item[\"output\"]\n","            }\n","            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n","\n","\n","# Provide the path where you want to save the formatted dataset\n","process_jsonl_file(dataset, \"./training_datasett.jsonl\")"]},{"cell_type":"code","execution_count":null,"id":"438b6a00-6a5d-4694-8dc9-de1735fafcdc","metadata":{"colab":{"referenced_widgets":["1c898b8a704a43f2805dbfed3b26f168"]},"id":"438b6a00-6a5d-4694-8dc9-de1735fafcdc","outputId":"a1a95038-6fc7-4eb4-e5b4-8ca844c05b00"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c898b8a704a43f2805dbfed3b26f168","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = load_dataset('json', data_files='./training_datasett.jsonl' , split='train')\n"]},{"cell_type":"code","execution_count":null,"id":"938d739c-67b5-4ee9-9a5f-dbd20aecba40","metadata":{"id":"938d739c-67b5-4ee9-9a5f-dbd20aecba40","outputId":"103661b2-8411-4544-f16f-ffb935ef54b2"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'instruction', 'input', 'output'],\n","        num_rows: 49\n","    })\n","    test: Dataset({\n","        features: ['text', 'instruction', 'input', 'output'],\n","        num_rows: 49\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import DatasetDict\n","\n","indices = range(0,49)\n","\n","dataset_dict = {\"train\": train_dataset.select(indices),\n","                \"test\": train_dataset.select(indices)}\n","\n","raw_datasets = DatasetDict(dataset_dict)\n","raw_datasets\n","\n"]},{"cell_type":"code","execution_count":null,"id":"31edbb41-5ee3-4226-8afe-9893fa07b114","metadata":{"id":"31edbb41-5ee3-4226-8afe-9893fa07b114","outputId":"cc50c12e-2426-472d-efa2-7fd6d6f6776c"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['text', 'instruction', 'input', 'output'])\n"]}],"source":["example = raw_datasets[\"train\"][0]\n","print(example.keys())"]},{"cell_type":"code","execution_count":null,"id":"0161e56e-2bff-45e1-9bb6-6e7a519c8ff3","metadata":{"id":"0161e56e-2bff-45e1-9bb6-6e7a519c8ff3","outputId":"19a0c4d3-22bc-41b2-bdb7-1bbd46ad2433"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample 0 of the processed training set:\n","\n","<s>[INST] What is the crypto-asset project description of Algorand here are the inputs Algorand is a blockchain-based cryptocurrency protocol that aims to create a decentralized, secure, and scalable platform for diverse applications. Founded by Silvio Micali, it employs a unique Pure Proof of Stake (PPoS) consensus mechanism, ensuring security and decentralization by randomly selecting validators from the pool of token holders. Algorand supports high transaction throughput with low latency, making it suitable for fast-processing applications like finance and gaming. The platform offers robust smart contract capabilities, developer-friendly tools, and a commitment to sustainability through energy-efficient operations. Its decentralized governance model and interoperability features further enhance its appeal for building and integrating various blockchain solutions. Additionally, Algorand supports NFTs and other digital assets, broadening its utility in the digital economy. The Algorand Foundation promotes the ecosystem's growth and development through funding, research, and community engagement. [/INST] \\n The Algorand blockchain uses a decentralised Byzantine agreement protocol that uses pure proof of stake (PPOS), which selects validators randomly and secretly, making it highly resistant to attacks and ensuring rapid transaction finality. Algorand supports a wide range of services and applications, in areas of DeFi, NFTs, smart contracts, Decentralised applications, enterprise solutions and ecosystem support. Finally, Algorand is governed by a decentralised community governance model facilitated by the Algorand Foundation. </s>\n","Sample 7 of the processed training set:\n","\n","<s>[INST] What is the fixed supply of Algorand? here are the inputs The fixed supply of Algorand (ALGO) is 10 billion tokens. This total supply is hard-capped, meaning no more tokens will be created beyond this limit. The distribution of these tokens is planned to occur over a period of time, with specific allocations for various purposes such as rewards, community incentives, and network operations. [/INST] \\n At the genesis of the Algorand blockchain, 10 billion ALGO was minted. This cap was established at the launch of the Algorand blockchain and is immutable, ensuring that no more than 10 billion ALGO tokens will ever exist. </s>\n","Sample 37 of the processed training set:\n","\n","<s>[INST] What are the key development phases of Cardano? here are the inputs  [/INST] \\n Development Phases Cardanoâ€™s development phases include Byron, Shelley, Goguen, Basho, and Voltaire â€“ all named after poets except for Goguen, a computer scientist. Byron: Byron set the foundation for Cardano development allowing users to buy and sell ada on a proof-of-stake blockchain network. Initially, the Cardano ledger was established as a federated network, where block production and transaction validation were maintained by the founding entities. Byron saw the delivery of Daedalus and Yoroi wallets, and also provided users with a Block Explorer â€’ a tool specifically designed for browsing the blockchain. Shelley: The Shelley development theme introduced a decentralised ledger creating a completely new economic system, which drives the networkâ€™s growth and gradual optimization. Shelley evolved from Byronâ€™s federated network maintenance, with more and more blocks being produced by the distributed stake pool operator community. This theme focused on many critical steps that ensure enhanced user experience in terms of stake pool operation, delegation preferences, and incentives. Goguen: Goguen development focused on the establishment of a global, financial, and multi-functional system for decentralised application (DApp) building, smart contract support, and custom token issuance. Goguen is a key building block to establish a versatile platform to build solutions around such application domains as supply chain, track and trace, finance, medical records, identity voting, property registration, P2P payments, and many others. Basho: Basho focuses on Cardanoâ€™s optimization in terms of improving the scalability and interoperability of the network. Whereas other development stages focus on decentralisation and new functionality, Basho is about improving the underlying performance of the Cardano network to better support growth and adoption for applications with high transaction volume. Ledger eras: There are several eras within the evolution of Cardano. Each era refers to the rules of the ledger. For example, what transaction types and what data is stored in the ledger, or the validity and meaning of the transactions. Byron and Shelley eras: The evolution of the Cardano mainnet began with the Byron ledger rules. The mainnet underwent a hard fork in late July 2020 to switch from the Byron rules to the Shelley ledger rules. It was a full reimplementation of Cardano, which enabled two fundamental changes: the support for multiple sets of ledger rules, and the management of the hard fork process of switching from one set of rules to the next. In other words, the new implementation could support both the Byron rules and the Shelley rules, which meant that, when deployed to the mainnet in early 2020, the implementation was fully compatible with the Byron rules. This allowed for a smooth transition from the old to the new implementation. Once all Cardano users had upgraded their nodes to the new implementation, it became possible to invoke the hard fork combinator event and switch to the Shelley rules. Allegra, Mary, and Alonzo eras: Allegra, Mary, and Alonzo eras are all part of the Goguen development phase. Starting with Goguen, the ledger team introduced the notion of era into the ledger code. Shelley ledger rules then became â€˜the Shelley eraâ€™. Because Goguen features were implemented in steps, each set of functionality was introduced with a different hard fork, hence there were several ledger eras: Allegra: introduced token locking support Mary: brought native tokens and multi-asset functionality to Cardano Alonzo: introduced smart contract support. The names Allegra and Mary were chosen for their connection to the poet Percy Shelley and were only intended to be used as variable names for a very specific abstraction used in the ledger code. Goguen, the smart contract development phase, was the only phase named not after a poet. So the name of the ledger era that introduced smart contracts was named after Alonzo Church â€“ the person who invented the lambda calculus (Plutus Core uses a variant of system F). Going forward, the teams decided to use names in a,b,c order, after individuals who contributed to mathematics and computer science. One lack of consistency to notice is that eras can use both first and last names. This is driven by conciseness. Babbage era: The Babbage ledger era introduced such features as inline datums, reference scripts, and reference inputs. However, the release is also known as Vasil, named to honour the late Bulgarian mathematician and Cardano ambassador Vasil Dabov. The future plans for Cardano: Voltaire era: Decentralised governance and decision making lie at the heart of Voltaire granting the Cardano community the ability to vote on network development updates, technical improvements, and project funding. For the Cardano network to become more decentralised, it requires not only the distributed infrastructure introduced during Shelley but also the capacity to be maintained and improved over time in a decentralised way. </s>\n"]}],"source":["import random\n","\n","# create the splits\n","train_dataset = raw_datasets[\"train\"]\n","eval_dataset = raw_datasets[\"test\"]\n","\n","for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n","  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"]},{"cell_type":"markdown","id":"46da0d5b-df08-4b1d-9b14-2f83482060ee","metadata":{"id":"46da0d5b-df08-4b1d-9b14-2f83482060ee"},"source":["## Loading Mistral model"]},{"cell_type":"code","execution_count":null,"id":"d2b73afa-ec8f-41df-9ed6-a3e8244f6b0e","metadata":{"colab":{"referenced_widgets":["67f05ad51510435a9b93d6f107ac168d","63028833c0814d929a847b21e339c971","00f79fb610ec4a3f9dbdc0cb79a06b6f","9aebfce65f13493cae7c097ca48a3858"]},"id":"d2b73afa-ec8f-41df-9ed6-a3e8244f6b0e","outputId":"ca50f2bb-dc51-47b0-93dd-96d6f8609d2e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67f05ad51510435a9b93d6f107ac168d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"63028833c0814d929a847b21e339c971","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00f79fb610ec4a3f9dbdc0cb79a06b6f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9aebfce65f13493cae7c097ca48a3858","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# set pad_token_id equal to the eos_token_id if not set\n","if tokenizer.pad_token_id is None:\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Set reasonable default for models without max length\n","if tokenizer.model_max_length > 100_000:\n","  tokenizer.model_max_length = 2048\n"]},{"cell_type":"markdown","id":"ece3f9db-431f-4a63-9399-10edcea02548","metadata":{"id":"ece3f9db-431f-4a63-9399-10edcea02548"},"source":["### Define model arguments for LoRA"]},{"cell_type":"code","execution_count":null,"id":"c118fa59-1249-4d53-bbc6-e78965271bdb","metadata":{"id":"c118fa59-1249-4d53-bbc6-e78965271bdb"},"outputs":[],"source":["from transformers import BitsAndBytesConfig\n","import torch\n","\n","# specify how to quantize the model\n","quantization_config = BitsAndBytesConfig(\n","            load_in_4bit=True, # Load the model in 4-bit precision to save memory and improve performance\n","            bnb_4bit_quant_type=\"nf4\", # Use NormalFloat 4 (nf4) quantization, which is more efficient than standard 4-bit quantization\n","            bnb_4bit_compute_dtype=\"float16\", # Set computation precision to 16-bit floating point, balancing performance and precision\n",")\n","device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n","\n","model_kwargs = dict(\n","    attn_implementation=\"flash_attention_2\", # Flash Attention drastically speeds up model computations\n","    torch_dtype=\"auto\",\n","    use_cache=False, # False as we're going to use gradient checkpointing\n","    device_map=device_map,\n","    quantization_config=quantization_config,\n",")"]},{"cell_type":"markdown","id":"1ae69d98-761b-4966-ae93-a6ca4b8cfb05","metadata":{"id":"1ae69d98-761b-4966-ae93-a6ca4b8cfb05"},"source":["Loading model"]},{"cell_type":"code","execution_count":null,"id":"87672c16-5e1d-4c47-aca1-d4a353a73edf","metadata":{"colab":{"referenced_widgets":["fe67b63ed99b40afa7c7eb728f16dd3d"]},"collapsed":true,"id":"87672c16-5e1d-4c47-aca1-d4a353a73edf","outputId":"8d8c5708-e009-4c67-b07d-d7d83643b737"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/config.json\n","Model config MistralConfig {\n","  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.43.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/model.safetensors.index.json\n","Instantiating MistralForCausalLM model under default dtype torch.float16.\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe67b63ed99b40afa7c7eb728f16dd3d","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing MistralForCausalLM.\n","\n","All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]}],"source":["model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map={\"\":0})"]},{"cell_type":"markdown","id":"7c6f68a4-2d83-46c6-9197-4dbe5adbffb7","metadata":{"id":"7c6f68a4-2d83-46c6-9197-4dbe5adbffb7"},"source":["## Define SFT Trainer"]},{"cell_type":"code","execution_count":null,"id":"632a89cb-5e4d-48c1-b9c6-8d018bc5a32f","metadata":{"collapsed":true,"id":"632a89cb-5e4d-48c1-b9c6-8d018bc5a32f","outputId":"e1430836-a6e0-4b64-9af1-723369c6d9ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","PyTorch: setting up devices\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","Using auto half precision backend\n"]}],"source":["from trl import SFTTrainer\n","from peft import LoraConfig\n","from transformers import TrainingArguments\n","\n","# path where the Trainer will save its checkpoints and logs\n","output_dir = './mistral-7b-instruct-lora-v3.0'\n","\n","# based on config\n","training_args = TrainingArguments(\n","    fp16=True,\n","    do_eval=True,\n","    evaluation_strategy=\"epoch\",\n","    gradient_accumulation_steps=3,\n","    gradient_checkpointing=True,\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    learning_rate=1e-5, # Set the learning rate to a small value (0.00001) to ensure gradual and stable updates during fine-tuning\n","    log_level=\"info\", # Set the logging level to 'info' to capture general information during the training process\n","    logging_steps=10, # Log training information every 10 steps to monitor progress\n","    logging_strategy=\"steps\", # Log based on the number of steps rather than time intervals\n","    lr_scheduler_type=\"cosine\", # Use a cosine annealing schedule for the learning rate, which gradually reduces the learning rate following a cosine curve\n","    max_steps=-1, # Set the maximum number of training steps; -1 means no limit, and the number of steps is determined by the number of epochs\n","    num_train_epochs=10, # Set the total number of training epochs to 10, meaning the model will iterate over the entire dataset 10 times\n","    output_dir=output_dir,\n","    optim=\"paged_adamw_8bit\",\n","    overwrite_output_dir=True,\n","    per_device_eval_batch_size=1, # originally set to 8\n","    per_device_train_batch_size=1, # originally set to 8\n","    push_to_hub=True, #\n","    hub_model_id=\"mistral-7b-instruct-lora-v3.0\", #Name of the model\n","    hub_strategy=\"every_save\", #\n","    save_strategy=\"no\",\n","    save_total_limit=None,\n","    seed=42,\n",")\n","\n","# based on config\n","peft_config = LoraConfig(\n","        r=8,\n","        lora_alpha=16,\n","        lora_dropout=0.1,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",")\n","\n","trainer = SFTTrainer(\n","        model=model,\n","        #model_init_kwargs=model_kwargs,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        dataset_text_field=\"text\",\n","        tokenizer=tokenizer,\n","        packing=True,\n","        peft_config=peft_config,\n","        max_seq_length=tokenizer.model_max_length,\n","    )"]},{"cell_type":"code","execution_count":null,"id":"3d03e414-c0e4-4eeb-b2d4-1c5a2864bf97","metadata":{"id":"3d03e414-c0e4-4eeb-b2d4-1c5a2864bf97","outputId":"19b0a7b9-fca8-40ad-e823-c16be6508351"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 12\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 1\n","  Total train batch size (w. parallel, distributed & accumulation) = 3\n","  Gradient Accumulation steps = 3\n","  Total optimization steps = 40\n","  Number of trainable parameters = 6,815,744\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [40/40 03:57, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.367471</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>2.337193</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.361200</td>\n","      <td>2.308207</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.361200</td>\n","      <td>2.281873</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.296000</td>\n","      <td>2.259811</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.296000</td>\n","      <td>2.243010</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>2.296000</td>\n","      <td>2.231699</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.214100</td>\n","      <td>2.225432</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>2.214100</td>\n","      <td>2.222899</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.252000</td>\n","      <td>2.222434</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 12\n","  Batch size = 1\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=40, training_loss=2.280847501754761, metrics={'train_runtime': 242.5221, 'train_samples_per_second': 0.495, 'train_steps_per_second': 0.165, 'total_flos': 1.04951451746304e+16, 'train_loss': 2.280847501754761, 'epoch': 10.0})"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"127cbf29-b3ca-4264-aa92-b841eef0e74b","metadata":{"id":"127cbf29-b3ca-4264-aa92-b841eef0e74b"},"outputs":[],"source":["\n","trainer.save_state()"]},{"cell_type":"markdown","id":"e2194e10-674a-462f-b9bf-98e173a0d693","metadata":{"id":"e2194e10-674a-462f-b9bf-98e173a0d693"},"source":["## Evaluate model with prompt"]},{"cell_type":"code","execution_count":null,"id":"8a5dad17-92c7-4cd1-9f2b-161ac03b553c","metadata":{"id":"8a5dad17-92c7-4cd1-9f2b-161ac03b553c","outputId":"937dc598-03ac-469f-d9d8-27a6edb44b84"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<s><s>[INST] What are the resource allocations for the Chainlink network? here are the inputs During the initial coin offering (ICO) for LINK in September 2017, Chainlink announced a total and maximum supply of 1,000,000,000 LINK tokens. The current supply is about 453,509,553 LINK tokens, or about 45% of the total supply, as of end-September 2021. The Chainlink price at ICO was $0.11 and a total of 350 million LINK tokens were sold. This represents an over 200X from the ICO price to Chainlink price today.\n","\n","Chainlink price experienced a massive bull run in the period around mid-2019 to mid-2020. Chainlink bulls were colloquially referred to as â€œLINK Marines,'' becoming a well-known meme in the crypto community. Chainlink price reached an all-time high of $52.88 on May 9, 2021, on the back of an overall crypto market rally, as well as ongoing developments in the Chainlink ecosystem.\n","\n","According to the ICO documentation, 35% of the total token supply will go towards node operators and the incentivization of the ecosystem. Another 35% of LINK tokens were distributed during public sale events. Lastly, the remaining 30% of the total token supply was directed towards the company for the continued development of the Chainlink ecosystem and network. [/INST] Based on the information provided, here is a breakdown of the resource allocations for the Chainlink network:\n","\n","1. Node operators and ecosystem incentivization: 35% of the total token supply, which is equal to 350,000,000 LINK tokens.\n","2. Public sale events: 35% of the total token supply, which is equal to 350,000,000 LINK tokens.\n"]}],"source":["text = \"\"\"<s>[INST] What are the resource allocations for the Chainlink network? here are the inputs During the initial coin offering (ICO) for LINK in September 2017, Chainlink announced a total and maximum supply of 1,000,000,000 LINK tokens. The current supply is about 453,509,553 LINK tokens, or about 45% of the total supply, as of end-September 2021. The Chainlink price at ICO was $0.11 and a total of 350 million LINK tokens were sold. This represents an over 200X from the ICO price to Chainlink price today.\n","\n","Chainlink price experienced a massive bull run in the period around mid-2019 to mid-2020. Chainlink bulls were colloquially referred to as â€œLINK Marines,'' becoming a well-known meme in the crypto community. Chainlink price reached an all-time high of $52.88 on May 9, 2021, on the back of an overall crypto market rally, as well as ongoing developments in the Chainlink ecosystem.\n","\n","According to the ICO documentation, 35% of the total token supply will go towards node operators and the incentivization of the ecosystem. Another 35% of LINK tokens were distributed during public sale events. Lastly, the remaining 30% of the total token supply was directed towards the company for the continued development of the Chainlink ecosystem and network. [/INST]\"\"\"\n","\n","# Define the device for model inference\n","device = \"cuda:0\"\n","\n","# Tokenize the input text\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","# Generate output based on the input\n","outputs = model.generate(**inputs, max_new_tokens=100)\n","\n","# Decode and print the generated output\n","print(tokenizer.decode(outputs[0], skip_special_tokens=False))"]},{"cell_type":"code","execution_count":null,"id":"0867c54f-8e77-48e9-8c09-23efc30af459","metadata":{"id":"0867c54f-8e77-48e9-8c09-23efc30af459","outputId":"91107aca-6bc2-469e-f8d4-8ea0d9d590e7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<s> What are the organisations and people involved in the development of Ripple?\n","\n","Ripple is an open-source, decentralized payment protocol that enables the transfer of various types of assets, including cryptocurrencies, fiat currencies, and other commodities. The Ripple protocol was initially developed by a company called OpenCoin, which was later renamed Ripple Labs.\n","\n","Ripple Labs was founded in 2012 by Chris Larsen and Jed McCaleb. Larsen served as the CEO\n"]}],"source":["text = \"\"\"What are the organisations and people involved in the development of Ripple?\"\"\"\n","\n","# Define the device for model inference\n","device = \"cuda:0\"\n","\n","# Tokenize the input text\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","# Generate output based on the input\n","outputs = model.generate(**inputs, max_new_tokens=100)\n","\n","# Decode and print the generated output\n","print(tokenizer.decode(outputs[0], skip_special_tokens=False))"]},{"cell_type":"markdown","id":"2d02bc8d-bb98-4aa5-b767-1744be8af9b8","metadata":{"id":"2d02bc8d-bb98-4aa5-b767-1744be8af9b8"},"source":["## Saving model"]},{"cell_type":"code","execution_count":null,"id":"9ba95d3a-cc56-42c6-9a0d-14b2c5b2d1a0","metadata":{"colab":{"referenced_widgets":["5cdb8d08b29f46ffa0959b22e93b0eda","74a5b86fa83d437a9388fd73c455e20f","3b6e606ed8d9483685e2351f9c05e60f"]},"collapsed":true,"id":"9ba95d3a-cc56-42c6-9a0d-14b2c5b2d1a0","outputId":"001b8ef8-94ee-4add-9ad5-d50ac46673d7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to mistral-crypto-2.0\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/config.json\n","Model config MistralConfig {\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.43.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in mistral-crypto-2.0/tokenizer_config.json\n","Special tokens file saved in mistral-crypto-2.0/special_tokens_map.json\n","Saving model checkpoint to ./mistral-7b-instruct-lora-TRYING\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/config.json\n","Model config MistralConfig {\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.43.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in ./mistral-7b-instruct-lora-TRYING/tokenizer_config.json\n","Special tokens file saved in ./mistral-7b-instruct-lora-TRYING/special_tokens_map.json\n","Dropping the following result as it does not have all the necessary fields:\n","{'dataset': {'name': 'generator', 'type': 'generator', 'config': 'default', 'split': 'train', 'args': 'default'}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cdb8d08b29f46ffa0959b22e93b0eda","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74a5b86fa83d437a9388fd73c455e20f","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b6e606ed8d9483685e2351f9c05e60f","version_major":2,"version_minor":0},"text/plain":["training_args.bin:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.save_model(\"mistral-crypto-2.0\")"]},{"cell_type":"markdown","id":"011c6219-5a22-453b-86ee-7122426d3c73","metadata":{"id":"011c6219-5a22-453b-86ee-7122426d3c73"},"source":["## Testing huggingface model"]},{"cell_type":"code","execution_count":null,"id":"f54c25f1-8fe3-4ed1-bd11-74a0460148a4","metadata":{"colab":{"referenced_widgets":["44c52636f3d647949e9a6591a12be968","20fdbf60894c49b2a5ee105260e9bc46","21748567383744a397c093f1a602042e","c6c621acf4dc4b0fb20012822f91c332","52589a8d64554d848c250cd7ba8f68f3","373cc665b96349069c94df14a29cd3b7"]},"id":"f54c25f1-8fe3-4ed1-bd11-74a0460148a4","outputId":"ba6b9312-20cd-4428-8b58-5fff3472530c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44c52636f3d647949e9a6591a12be968","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20fdbf60894c49b2a5ee105260e9bc46","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21748567383744a397c093f1a602042e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading file tokenizer.model from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mirko5301--mistral-7b-instruct-lora/snapshots/22df6a57498aec525465401ebe2429e4829bbf81/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mirko5301--mistral-7b-instruct-lora/snapshots/22df6a57498aec525465401ebe2429e4829bbf81/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mirko5301--mistral-7b-instruct-lora/snapshots/22df6a57498aec525465401ebe2429e4829bbf81/tokenizer_config.json\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6c621acf4dc4b0fb20012822f91c332","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/config.json\n","Model config MistralConfig {\n","  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.43.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/model.safetensors.index.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52589a8d64554d848c250cd7ba8f68f3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing MistralForCausalLM.\n","\n","All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"373cc665b96349069c94df14a29cd3b7","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["What is Algorand? Algorand is an open-source, decentralized, and blockchain-based platform designed for building decentralized applications (dApps) and financial services. It was created by MIT professor Silvio Micali and his team\n"]}],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"mirko5301/mistral-7b-instruct-lora\")\n","model = AutoModelForCausalLM.from_pretrained(\"mirko5301/mistral-7b-instruct-lora\")\n","\n","text = \"\"\"What is Algorand?\"\"\"\n","\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","\n","\n","outputs = model.generate(**inputs, max_new_tokens=50)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"id":"8db884df-cba7-46af-abda-9a00ba71d271","metadata":{"id":"8db884df-cba7-46af-abda-9a00ba71d271"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}