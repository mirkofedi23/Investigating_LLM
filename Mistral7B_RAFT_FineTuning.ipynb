{"cells":[{"cell_type":"markdown","source":["# Mistral 7B RAFT Fine-tuning Script\n","### Contents:\n","- Loading Model from huggingface\n","- Loading Dataset (Synthetic Dataset) from huggingface\n","- Pre-process the training dataset with the Mistral formatting template\n","- Define LoRA model arguments and SFT Trainer\n","- Train the model , recording using Wand B\n","- Referenced Code from\n","  - https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb\n"],"metadata":{"id":"-avm1ZXte0tb"},"id":"-avm1ZXte0tb"},{"cell_type":"markdown","source":["### Installing Dependencies"],"metadata":{"id":"phJbalQmfXgZ"},"id":"phJbalQmfXgZ"},{"cell_type":"code","execution_count":null,"id":"75aa92c7-fff7-4711-b7fe-1110995fb039","metadata":{"id":"75aa92c7-fff7-4711-b7fe-1110995fb039","outputId":"75201a44-0650-41f3-aabc-7cca61b85ec0"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install -q transformers[torch] datasets\n","!pip install -q bitsandbytes trl peft\n","!pip install flash-attn --no-build-isolation\n","!pip install -q torch\n","!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n","!pip install -q datasets #huggingface datasets to download and manipulate datasets\n","!pip install -q peft #Parameter efficient finetuning - for qLora Finetuning\n","!pip install -q bitsandbytes #For Model weights quantisation\n","!pip install -q trl #Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning\n","!pip install -q wandb -U #Used to monitor the model score during training\n","!pip install --upgrade huggingface_hub\n","!pip install wandb\n","!huggingface-cli login --token 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\n","\n","\n","import json\n","import pandas as pd\n","import torch\n","import wandb\n","from datasets import Dataset, load_dataset\n","from huggingface_hub import notebook_login\n","from peft import LoraConfig, PeftModel\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from trl import SFTTrainer\n"]},{"cell_type":"markdown","source":["### Logging into Huggingface and WandB"],"metadata":{"id":"2QJbS5XofbnN"},"id":"2QJbS5XofbnN"},{"cell_type":"code","execution_count":null,"id":"96820a17-2914-4c13-a128-24c5b9872f2b","metadata":{"colab":{"referenced_widgets":["ec7bbd5070c7400e85d294e6bbff5a28","beb8cc3acf83403989cd184eb6fd3321"]},"id":"96820a17-2914-4c13-a128-24c5b9872f2b","outputId":"eef748f5-024f-479e-f36c-e2855692c9e0","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.4)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (3.11.0)\n","Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.27.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (68.2.2)\n","Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"beb8cc3acf83403989cd184eb6fd3321","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["\n","hf_token = #\n","wb_token = #\n","wandb.login(key=wb_token)\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"id":"ecd252ca-77b8-47f9-bc5b-9d0516a1df7d","metadata":{"id":"ecd252ca-77b8-47f9-bc5b-9d0516a1df7d","colab":{"referenced_widgets":["204dcfa0d66442c2b1762a144019c6c2","07ee37e9e39a4bdc9a111df8b67a18a8"]},"outputId":"84e57bbd-250e-4977-9131-e1f39e7fbd27"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"204dcfa0d66442c2b1762a144019c6c2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/3.92M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07ee37e9e39a4bdc9a111df8b67a18a8","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/264 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"mirko5301/raft_trying\", split=\"train\")"]},{"cell_type":"markdown","source":["### Formatting the dataset with the Mistral template formatting"],"metadata":{"id":"cfsq2ORcfi99"},"id":"cfsq2ORcfi99"},{"cell_type":"code","execution_count":null,"id":"bf826db5-a907-45a8-b878-9cf17b7d2771","metadata":{"id":"bf826db5-a907-45a8-b878-9cf17b7d2771"},"outputs":[],"source":["# Function to format each row in the dataset\n","def create_text_row(instruction, output, input):\n","    text_row = f\"\"\"<s>[INST] {instruction} here are the inputs {input} [/INST] \\\\n {output} </s>\"\"\"\n","    return text_row\n","\n","# Iterate over all the rows, format the dataset, and store it in a JSONL file\n","def process_jsonl_file(dataset, output_file_path):\n","    with open(output_file_path, \"w\") as output_jsonl_file:\n","        for item in dataset:\n","            json_object = {\n","                \"text\": create_text_row(item[\"question\"], item[\"cot_answer\"], item[\"instruction\"]),\n","                \"question\": item[\"question\"],\n","                \"instruction\": item[\"instruction\"],\n","                \"cot_answer\": item[\"cot_answer\"]\n","            }\n","            output_jsonl_file.write(json.dumps(json_object) + \"\\n\")\n","\n","\n","# Provide the path where you want to save the formatted dataset\n","process_jsonl_file(dataset, \"./training_datasett.jsonl\")"]},{"cell_type":"code","execution_count":null,"id":"438b6a00-6a5d-4694-8dc9-de1735fafcdc","metadata":{"colab":{"referenced_widgets":["9b6a692c5eba4e048e3e5897dfed59ca","c875d320db324b29b2fcc892eb565dc6"]},"id":"438b6a00-6a5d-4694-8dc9-de1735fafcdc","outputId":"ecee26ad-7329-483b-9101-67c67cd81dfc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c875d320db324b29b2fcc892eb565dc6","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_dataset = load_dataset('json', data_files='./training_datasett.jsonl' , split='train')\n"]},{"cell_type":"code","execution_count":null,"id":"45fdc7c9-6f63-4ab1-a78f-d6816da0aa36","metadata":{"id":"45fdc7c9-6f63-4ab1-a78f-d6816da0aa36","outputId":"8d596c18-cbf3-4945-b350-c2bed6e13ed5"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text', 'question', 'instruction', 'cot_answer'],\n","    num_rows: 264\n","})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"code","execution_count":null,"id":"938d739c-67b5-4ee9-9a5f-dbd20aecba40","metadata":{"id":"938d739c-67b5-4ee9-9a5f-dbd20aecba40","outputId":"de95b991-1340-412f-eb6a-cc6aed02b143"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'question', 'instruction', 'cot_answer'],\n","        num_rows: 264\n","    })\n","    test: Dataset({\n","        features: ['text', 'question', 'instruction', 'cot_answer'],\n","        num_rows: 264\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import DatasetDict\n","\n","indices = range(0,264)\n","\n","dataset_dict = {\"train\": train_dataset.select(indices),\n","                \"test\": train_dataset.select(indices)}\n","\n","raw_datasets = DatasetDict(dataset_dict)\n","raw_datasets\n","\n","#train_dataset=train_dataset,\n","    #    eval_dataset=eval_dataset,"]},{"cell_type":"code","execution_count":null,"id":"31edbb41-5ee3-4226-8afe-9893fa07b114","metadata":{"id":"31edbb41-5ee3-4226-8afe-9893fa07b114","outputId":"2ceda21d-e4ef-4dcd-a97f-dc98b0895d7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['text', 'question', 'instruction', 'cot_answer'])\n"]}],"source":["example = raw_datasets[\"train\"][0]\n","print(example.keys())"]},{"cell_type":"code","execution_count":null,"id":"0161e56e-2bff-45e1-9bb6-6e7a519c8ff3","metadata":{"id":"0161e56e-2bff-45e1-9bb6-6e7a519c8ff3","outputId":"6b5fad44-d4d4-4118-db38-70d976c4fc12"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample 44 of the processed training set:\n","\n","<s>[INST] How is the Cardano network secured and how does it compare to Bitcoin in terms of energy efficiency? here are the inputs <DOCUMENT>What Is Cardano (ADA): Cardano is a proof-of-stake blockchain platform that says its goal is to allow “changemakers, innovators and visionaries” to bring about positive global change. To learn more about this project, check out our deep dive of Cardano. The open-source project also aims to “redistribute power from unaccountable structures to the margins to individuals” — helping to create a society that is more secure, transparent and fair. Cardano was founded back in 2017, and named after the 16th century Italian polymath Gerolamo Cardano. The native ADA token takes its name from the 19th century mathematician Ada Lovelace, widely regarded as the world’s first computer programmer. The ADA token is designed to ensure that owners can participate in the operation of the network. Because of this, those who hold the cryptocurrency have the right to vote on any proposed changes to the software. </DOCUMENT>\n","<DOCUMENT>The team behind the layered blockchain say that there have already been some compelling use cases for its technology, which aims to allow decentralized apps and smart contracts to be developed with modularity. In August 2021, Charles Hoskinson announced the launch of the Alonzo hard fork, causing Cardano price to surge, gaining 116% in the following month. On Sept. 12, 2021, the Cardano ‘Alonzo’ hard fork officially launched, bringing smart contract functionality to the blockchain. Over 100 smart contracts were deployed in the following 24 hours after the launch. Cardano is used by agricultural companies to track fresh produce from field to fork, while other products built on the platform allow educational credentials to be stored in a tamper-proof way, and retailers to clamp down on counterfeit goods.\n","\n","Who Are the Founders of Cardano: Cardano was founded by Charles Hoskinson, who was also one of the co-founders of the Ethereum network. He is the CEO of IOHK, the company that built Cardano’s blockchain. In an interview for CoinMarketCap’s Crypto Titans series, Hoskinson said that he got involved in cryptocurrencies back in 2011 — and dabbled in mining and trading. He explained that his first professional involvement in the industry came in 2013, when he created a course about Bitcoin that ended up being taken by 80,000 students. As well as being a technology entrepreneur, Hoskinson is also a mathematician. In 2020, his technology company donated ADA worth $500,000 to the University of Wyoming’s Blockchain Research and Development Lab.\n","\n","What Makes Cardano Unique: Cardano is one of the biggest blockchains to successfully use a proof-of-stake consensus mechanism, which is less energy intensive than the proof-of-work algorithm relied upon by Bitcoin. Although the much larger Ethereum is going to be upgrading to PoS, this transition is only going to take place gradually. The project has taken pride in ensuring that all of the technology developed goes through a process of peer-reviewed research, meaning that bold ideas can be challenged before they are validated. According to the Cardano team, this academic rigor helps the blockchain to be durable and stable — increasing the chance that potential pitfalls can be anticipated in advance. In 2020, Cardano held a Shelley upgrade that aimed to make its blockchain “50 to 100 times more decentralized” than other large blockchains. At the time, Hoskinson predicted that this would pave the way for hundreds of assets to run on its network. The Alonzo hard fork launch in September 2021 will bring an end to the Shelley era, and usher in the Goguen phase. Users can develop and deploy smart contracts on Cardano, allowing native decentralized applications (DApps) to be built on blockchain. Cardano price broke the $3 mark and hit an all-time high of $3.101 on Sept. 2, 2021, ahead of the launch.\n","\n","What Is Cardano’s Vasil Hard Fork: Named after late Bulgarian mathematician Vasil Dabov, a prominent contributor to Cardano, the Vasil hard fork is touted as one of the most highly-anticipated upgrades for Cardano. The hard fork is the third development epoch of Cardano and is supposed to introduce several upgrades to the blockchain’s smart contract programming language Plutus and the network’s capacity. The event was originally billed to happen in June 2022, but has been postponed a number of times. Vasil will introduce five critical mechanisms to improve Cardano’s scalability and usability — CIP-31, CIP-32, CIP-33, CIP-40 and diffusion pipelining. CIP-31, aka “reference inputs” will introduce a new kind of input that would allow developers to look at the result of an output without having to spend it. This would optimize transaction throughput and increase concurrency. The CIP-32 proposal aims to enable inline datums. Rather than attach datum to datum hashes, which is the current state of things, CIP-32 would allow developers to attach datums to outputs. By implementing this update, devs can code scripts that directly point to the input, making room for simpler and quicker communication of datum values between users. The Cardano Improvement Proposal 33 would allow reference scripts to be attached to outputs. As a result, the reference scripts are used to satisfy the validation requirements in place of the spending transaction. These reference scripts will make the validation process more efficient and reduce the size of transactions. Meanwhile, CIP-40 features a brand-new type of output to transactions called collateral outputs, aimed at improving the overall scalability of the network. Diffusing pipelining is Cardano’s consensus layer scaling solution. The improvement proposal will see more DApp deployment by overlaying some of the steps that a block needs to go through as it moves across the chain: this would allow for concurrent transactions.\n","\n","How Many Cardano (ADA) Coins Are There in Circulation: There is a maximum supply of 45 billion ADA — but at the time of writing, there was a circulating supply of about 31 billion. Five rounds of public sales of Cardano tokens were held between September 2015 and January 2017. Cardano price during its pre-launch sale was $0.0024, which represents an over 1000x return, given Cardano price now. Approximately 2.5 billion ADA was allotted to IOHK once the network launched. Meanwhile, an additional 2.1 billion ADA was given to Emurgo, a global blockchain technology company that served as a founding entity of the Cardano protocol. Last but not least, 648 million ADA was given to the not-for-profit Cardano Foundation, which aims to promote the platform and increase levels of adoption. Overall, about 16% of ADA’s total supply went to the project’s founders, with the remaining 84% being split among investors.\n","\n","How Is the Cardano Network Secured: Cardano is secured through an “environmentally sustainable, verifiably secure” PoS protocol that’s known as Ouroboros. The project says that Ouroboros improves upon the security guarantees that are delivered by a PoW consensus mechanism while using substantially less power — claiming that it is four times more energy efficient than Bitcoin. It is described as a blend of unique technology and mathematically verified mechanisms, with behavioral psychology and economic philosophy thrown in for good measure. </DOCUMENT>\n","<DOCUMENT>During each epoch, a fixed but parameterizable percentage of the remaining reserve is taken from the reserve and used for epoch rewards and treasury, where the amount being sent to the treasury is a fixed percentage of the amount taken from the reserve.\n","\n","About Ouroboros: Cardano runs on the Ouroboros consensus protocol, which was delivered with several peer-reviewed papers presented at top-tier conferences and publications in the area of cybersecurity and cryptography. Rather than relying on 'miners' (as in proof-of-work protocols) to solve computationally complex equations to create new blocks – and rewarding the first to do so – proof of stake selects stake pools to create new blocks based on the stake they control in the network. How Ouroboros works is that Ouroboros divides time on Cardano into epochs where each epoch is divided into slots. A slot is a short period of time in which a block can be created. Grouping slots into epochs is central to adjusting the leader election process to the dynamically changing stake distribution. Central to Ouroboros’ design is that it must retain its security in the presence of attacks. As such, the protocol has built-in tolerance to prevent attackers from propagating alternative versions of the blockchain and assumes that an adversary may send arbitrary messages to any participant at any time. The protocol is guaranteed to be secure in the so-called synchronous setting (that is, with strong guarantees on message delivery times) so long as more than 51% of the stake is controlled by honest participants (ie, those following the protocol). A slot leader is elected for each slot, who is responsible for adding a block to the chain and passing it to the next slot leader. To protect against adversarial attempts to subvert the protocol, each new slot leader is required to consider the last few blocks of the received chain as transient: only the chain that precedes the prespecified number of transient blocks is considered settled. This is also referred to as the settlement delay. Among other things, this means that a stakeholder can go offline and still be synced to the blockchain, so long as it’s not for more than the settlement delay. Within the Ouroboros protocol, each network node stores a copy of the transaction mempool – where transactions are added if they are consistent with existing transactions – and the blockchain. </DOCUMENT>\n","How is the Cardano network secured and how does it compare to Bitcoin in terms of energy efficiency? [/INST] \\n assistant: Step-by-step reasoning:\n","\n","1. The first part of the question asks how the Cardano network is secured. To answer this, we need to find information in the context about Cardano's security mechanism. The last paragraph provides this information.\n","\n","2. From the last paragraph, we can quote: ##begin_quote## \"Cardano is secured through an “environmentally sustainable, verifiably secure” PoS protocol that’s known as Ouroboros. The project says that Ouroboros improves upon the security guarantees that are delivered by a PoW consensus mechanism while using substantially less power.\" ##end_quote## This tells us that Cardano uses a proof-of-stake (PoS) protocol called Ouroboros for its security.\n","\n","3. The second part of the question asks how Cardano compares to Bitcoin in terms of energy efficiency. The same paragraph quoted above also provides this information.\n","\n","4. From the same paragraph, we can quote: ##begin_quote## \"The project says that Ouroboros improves upon the security guarantees that are delivered by a PoW consensus mechanism while using substantially less power — claiming that it is four times more energy efficient than Bitcoin.\" ##end_quote## This tells us that Cardano is more energy efficient than Bitcoin.\n","\n","Final Answer: The Cardano network is secured through a proof-of-stake protocol known as Ouroboros, which is environmentally sustainable and verifiably secure. Compared to Bitcoin, which uses a proof-of-work consensus mechanism, Cardano is four times more energy efficient. </s>\n","Sample 156 of the processed training set:\n","\n","<s>[INST] What is the purpose of the Hedera network nodes? here are the inputs <DOCUMENT>No license will be required to write software that\n","uses the services of the Hedera platform. No license will be required to build smart contracts on top of\n","the Hedera platform. Applications built upon the Hedera platform can be open source or proprietary.\n","They do not require any license or any approval from Hedera. Software developed using the platform APIs\n","will not be encumbered in any way. Software developers will have complete ownership and discretion on\n","the licensing they choose for their applications that use the Hedera platform.\n","Swirlds owns the intellectual property rights in the hashgraph consensus algorithm. The Hedera\n","Governing Council has a license from Swirlds to use the hashgraph consensus algorithm and associated\n","technology for the Hedera distributed public ledger platform. In exchange for that license, the Hedera\n","Governing Council will pay Swirlds 10% of network revenue (with monthly minimums) and Swirlds will\n","own 5% of Hedera coins. Swirlds will continue to require licenses for use of the hashgraph technology in\n","private, permissioned networks, but no license will be required for distributed applications that run on\n","Hedera’s public platform. Hedera and Swirlds will use the patent rights associated with the hashgraph\n","algorithm defensively to legally prohibit the forking of the codebase and the creation of a competing\n","platform and currency. Developers are free to build distributed applications on top of the Hedera\n","platform with associated native tokens.\n","In summary, Hedera will simultaneously embrace open review of the software code, while bringing\n","stability to the platform and cryptocurrency by controlling the license. In this way, Hedera will provide a\n","transparent codebase, assuring the stability that markets demand for mainstream adoption.\n","\n","\n","\n","\n","\n","\n","INTERNET LAYER\n","The Hedera network nodes are all computers on the internet, communicating by TCP/IP connections\n","protected by TLS encryption with ephemeral keys for perfect forward secrecy. Nodes are addressed by\n","IP address and port, rather than by symbolic names, so attacks on the DNS system will not affect the\n","network.\n","HASHGRAPH CONSENSUS LAYER\n","The nodes take transactions from clients and share them throughout the network with a gossip protocol.\n","Then all nodes run the hashgraph consensus algorithm to reach agreement on a consensus timestamp\n","for each transaction and its consensus order in history. Each node then applies the effects of the\n","transactions in consensus order to modify its copy of the shared state. In this way, all nodes maintain an\n","identical consensus state (within any given shard).\n","SERVICES LAYER\n","CRYPTOCURRENCY\n","The cryptocurrency is designed to be fast, which leads to low network fees, making very\n","small microtransactions practical. When the Hedera platform is running at scale, any user\n","will be able to run a node in the network and earn cryptocurrency payments for doing so.\n","Any user will be able to create an account by simply creating a key pair, without any name\n","or address attached to it. </DOCUMENT>\n","<DOCUMENT>Kearney, Bain & Company, and BNP Paribas. Lionel has an MBA from UC\n","Berkeley’s Haas School of Business, a Masters in Engineering from McGill\n","University, and a Bachelor’s degree in Engineering from Ecole des Mines.\n","MEHERNOSH (NOSH) MODY, SENIOR VICE-PRESIDENT, ENGINEERING\n","Nosh brings 25 years of experience as an engineering manager and\n","technology executive, and is the Senior Vice-President of Engineering\n","of Hedera. He was previously the Director of R&D at Trend Micro\n","TippingPoint, responsible for the development and innovation of\n","TippingPoint Intrusion Prevention Systems. Prior to Trend Micro, he\n","managed R&D at numerous startups, including Britestream Networks\n","and Coretrace Corp. Nosh has an MBA from the University of Texas and a\n","MS in Computer Science from the University of Massachusetts.\n","\n","\n","\n","ATUL MAHAMUNI, SENIOR VICE-PRESIDENT, PRODUCT\n","Atul has many years of experience in product management, and is the\n","Senior Vice-President of Product of Hedera. He was previously VicePresident of Blockchain SaaS Apps and Internet of Things (IoT) SaaS\n","Apps, PaaS Platform at Oracle. </DOCUMENT>\n","<DOCUMENT>Prior to that, he was Vice President and General Manager at\n","Cisco, where he co-launched, productized, and scaled Cisco’s fast-growing\n","Internet of Things (IoT) business. He has previously held roles at A.T.\n","</DOCUMENT>\n","What is the purpose of the Hedera network nodes? [/INST] \\n assistant: Step 1: Identify the relevant information in the context that directly relates to the question. The question asks about the purpose of the Hedera network nodes. The context provides information about the Hedera network nodes in the section titled \"INTERNET LAYER\" and \"HASHGRAPH CONSENSUS LAYER\".\n","\n","Step 2: Extract the necessary information from the context. \n","\n","##begin_quote##\n","\"The Hedera network nodes are all computers on the internet, communicating by TCP/IP connections protected by TLS encryption with ephemeral keys for perfect forward secrecy. Nodes are addressed by IP address and port, rather than by symbolic names, so attacks on the DNS system will not affect the network.\"\n","##end_quote##\n","\n","##begin_quote##\n","\"The nodes take transactions from clients and share them throughout the network with a gossip protocol. Then all nodes run the hashgraph consensus algorithm to reach agreement on a consensus timestamp for each transaction and its consensus order in history. Each node then applies the effects of the transactions in consensus order to modify its copy of the shared state. In this way, all nodes maintain an identical consensus state (within any given shard).\"\n","##end_quote##\n","\n","Step 3: Synthesize the information to form a comprehensive answer. The Hedera network nodes are computers on the internet that communicate using TCP/IP connections. They take transactions from clients and share them throughout the network. They then run the hashgraph consensus algorithm to reach agreement on a consensus timestamp for each transaction and its order in history. Each node applies the effects of the transactions in consensus order to modify its copy of the shared state, maintaining an identical consensus state.\n","\n","<ANSWER>: The purpose of the Hedera network nodes is to communicate transactions across the network, run the hashgraph consensus algorithm to reach agreement on the order and timestamp of each transaction, and maintain an identical consensus state by applying the effects of the transactions in consensus order. </s>\n","Sample 90 of the processed training set:\n","\n","<s>[INST] What is Chainlink? here are the inputs <DOCUMENT>What Is Chainlink (LINK)? Founded in 2017, Chainlink is a blockchain abstraction layer that enables universally connected smart contracts. Through a decentralized oracle network, Chainlink allows blockchains to securely interact with external data feeds, events and payment methods, providing the critical off-chain information needed by complex smart contracts to become the dominant form of digital agreement. The Chainlink Network is driven by a large open-source community of data providers, node operators, smart contract developers, researchers, security auditors and more. The company focuses on ensuring that decentralized participation is guaranteed for all node operators and users looking to contribute to the network.\n","\n","Who Are the Founders of Chainlink? Sergey Nazarov is a co-founder and CEO at Chainlink Labs. He graduated with a degree in business administration from New York University, with a focus on philosophy and administration. </DOCUMENT>\n","<DOCUMENT>His professional career began as a teaching fellow at NYU Stern School of Business. In 2009, Nazarov co-founded ExistLocal, a peer-to-peer marketplace for authentic local experiences. In 2014, he also co-founded CryptaMail, a completely decentralized, blockchain-based email service. In 2014, Nazarov teamed up with Steve Ellis and launched SmartContract, a platform that brings smart contracts to life by connecting them to external data and widely accepted bank payments. SmartContract was one of the entrepreneurial ventures that led Sergey Nazarov to the founding of Chainlink. Steve Ellis graduated with a degree in computer science from New York University in 2010. </DOCUMENT>\n","<DOCUMENT>The oracle problem revolves around a very simple limitation—blockchains cannot pull in data from or push data out to any external system as built-in functionality. As such, blockchains are isolated networks, akin to a computer with no Internet connection. The isolation of a blockchain is the precise property that makes it extremely secure and reliable, as the network only needs to form consensus on a very basic set of binary (true/false) questions using data already stored inside of its ledger. These questions include: did the public key holder sign the transaction with their corresponding private key, does the public address have enough funds to cover its transaction, and is the type of transaction valid within the particular smart contract? The very narrow focus of blockchain consensus is why smart contracts are referred to as being deterministic—they execute exactly as written with a much higher degree of certainty than traditional systems. However, for smart contracts to realize upwards of 90% of their potential use cases, they must connect to the outside world. For example, financial smart contracts need market information to determine settlements, insurance smart contracts need IoT and web data to make decisions on policy payouts, trade finance contracts need trade documents and digital signatures to know when to release payments, and many smart contracts want to settle in fiat currency on a traditional payment network. None of this information is inherently generated within the blockchain, nor are these traditional services directly accessible. Bridging the connection between the blockchain (on-chain) and the outside world (off-chain) requires an additional and separate piece of infrastructure known as an oracle.\n","\n","How Does Chainlink Solve the Oracle Problem? In order to bring determinism to the oracle layer, Chainlink has developed a network of decentralized oracle networks (DONs), with each DON involving a combination of multiple security techniques needed to service a particular use case. Open-source – being an open-source technology allows the wider blockchain community to independently verify the security and reliability of Chainlink’s source code and functions, as well as contribute to its improvement. External Adapters – allowing nodes to securely store API keys and manage account logins enables smart contracts to retrieve data from any external system and API, including those that are password/credential protected. Decentralization – employing decentralization at the node and data source level ensures no one node or data source is a single point of failure, providing users strong guarantees that data will be available, delivered on time, and resistant to manipulation. Data Signing – having nodes cryptographically sign the data they provide to smart contracts allows users to identify which nodes sent data and look at their past history to determine their performance quality. Service Agreements – using binding on-chain agreements between the requesting smart contract and the oracle provider that outline the terms of the oracle service and penalties/rewards for performance provides users with enforceable guarantees on the quality of their off-chain data requests. Reputation Systems – feeding signed on-chain data into reputation systems allows users to make informed decisions about which nodes are good and which nodes are not based on a variety of metrics like successful jobs performed, list of clients served, average response time, etc. Certification Services – enabling nodes to increase their security and reliability by obtaining any number of certifications can provide users additional guarantees like KYC, geographic location of the node, security reviews of their infrastructure, and more. Advanced Cryptography and Hardware – providing flexibility for more advanced cryptography (like zero-knowledge proofs) and hardware (such as trusted execution environments) enables oracles to perform additional functions like proving the origin of data (e.g. </DOCUMENT>\n","What is Chainlink? [/INST] \\n assistant: Step 1: Identify the key question, which is \"What is Chainlink?\" \n","\n","Step 2: Look for information in the context that directly answers this question. \n","\n","Step 3: The context provides a clear definition of Chainlink in the first few sentences: ##begin_quote## \"Founded in 2017, Chainlink is a blockchain abstraction layer that enables universally connected smart contracts. Through a decentralized oracle network, Chainlink allows blockchains to securely interact with external data feeds, events and payment methods, providing the critical off-chain information needed by complex smart contracts to become the dominant form of digital agreement. The Chainlink Network is driven by a large open-source community of data providers, node operators, smart contract developers, researchers, security auditors and more. The company focuses on ensuring that decentralized participation is guaranteed for all node operators and users looking to contribute to the network.\" ##end_quote##\n","\n","Step 4: Summarize this information into a succinct answer.\n","\n","<ANSWER>: Chainlink is a blockchain abstraction layer founded in 2017 that enables universally connected smart contracts. It allows blockchains to interact securely with external data feeds, events, and payment methods through a decentralized oracle network. The Chainlink Network is driven by a large open-source community and focuses on ensuring decentralized participation. </s>\n"]}],"source":["import random\n","\n","# create the splits\n","train_dataset = raw_datasets[\"train\"]\n","eval_dataset = raw_datasets[\"test\"]\n","\n","for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n","  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"]},{"cell_type":"markdown","id":"46da0d5b-df08-4b1d-9b14-2f83482060ee","metadata":{"id":"46da0d5b-df08-4b1d-9b14-2f83482060ee"},"source":["## Loading Mistral model"]},{"cell_type":"code","execution_count":null,"id":"d2b73afa-ec8f-41df-9ed6-a3e8244f6b0e","metadata":{"id":"d2b73afa-ec8f-41df-9ed6-a3e8244f6b0e","colab":{"referenced_widgets":["71198a2ea6c74fba9fbfb16dc877038b","e55999b4d7644a4ba95f9d75257b503b","a05ab4b5ca774ba6983ad5ba74da7f20","49fc0f4dfd95434483b61770416bc3cf"]},"outputId":"3412482a-6861-4a1f-9c70-a9364075bf64"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71198a2ea6c74fba9fbfb16dc877038b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e55999b4d7644a4ba95f9d75257b503b","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a05ab4b5ca774ba6983ad5ba74da7f20","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49fc0f4dfd95434483b61770416bc3cf","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","\n","# set pad_token_id equal to the eos_token_id if not set\n","if tokenizer.pad_token_id is None:\n","  tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Set reasonable default for models without max length\n","if tokenizer.model_max_length > 100_000:\n","  tokenizer.model_max_length = 2048\n"]},{"cell_type":"code","execution_count":null,"id":"7ab2ca27-57e5-4e22-8ecb-b5d66cc84a2d","metadata":{"id":"7ab2ca27-57e5-4e22-8ecb-b5d66cc84a2d"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ece3f9db-431f-4a63-9399-10edcea02548","metadata":{"id":"ece3f9db-431f-4a63-9399-10edcea02548"},"source":["### Define model arguments for LoRA and QLoRA"]},{"cell_type":"code","execution_count":null,"id":"c118fa59-1249-4d53-bbc6-e78965271bdb","metadata":{"id":"c118fa59-1249-4d53-bbc6-e78965271bdb"},"outputs":[],"source":["from transformers import BitsAndBytesConfig\n","import torch\n","\n","# specify how to quantize the model\n","quantization_config = BitsAndBytesConfig(\n","            load_in_4bit=True, # Load the model in 4-bit precision to save memory and improve performance\n","            bnb_4bit_quant_type=\"nf4\", # Use NormalFloat 4 (nf4) quantization, which is more efficient than standard 4-bit quantization\n","            bnb_4bit_compute_dtype=\"float16\", # Set computation precision to 16-bit floating point, balancing performance and precision\n",")\n","device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n","\n","model_kwargs = dict(\n","    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n","    torch_dtype=\"auto\",\n","    use_cache=False, # set to False as we're going to use gradient checkpointing\n","    device_map=device_map,\n","    quantization_config=quantization_config,\n",")"]},{"cell_type":"markdown","id":"1ae69d98-761b-4966-ae93-a6ca4b8cfb05","metadata":{"id":"1ae69d98-761b-4966-ae93-a6ca4b8cfb05"},"source":["Loading model"]},{"cell_type":"code","execution_count":null,"id":"87672c16-5e1d-4c47-aca1-d4a353a73edf","metadata":{"colab":{"referenced_widgets":["2600eb30dfc2490c8c9f770d6de9b512","0cffaedb46fa434b887cb1e58d963b6f","59d6ad1c180144ae8984631c6700ee06","be0fa1d01e584ad98be0d92fed52269c","182519af433b456bba898157b87e66ab","a84394b34549439f971ab0046e2c5083","d5a32175fe22416990998018d10423b5","7119993c35bd4d2db8ec35e3ff35a670","1e8ff7db46574ac584bc7e35f20e1f0e"]},"id":"87672c16-5e1d-4c47-aca1-d4a353a73edf","outputId":"e0f77be4-0897-4563-afe0-b902d70fb5f2","collapsed":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0cffaedb46fa434b887cb1e58d963b6f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59d6ad1c180144ae8984631c6700ee06","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be0fa1d01e584ad98be0d92fed52269c","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"182519af433b456bba898157b87e66ab","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a84394b34549439f971ab0046e2c5083","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5a32175fe22416990998018d10423b5","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7119993c35bd4d2db8ec35e3ff35a670","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e8ff7db46574ac584bc7e35f20e1f0e","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map={\"\":0})"]},{"cell_type":"markdown","id":"7c6f68a4-2d83-46c6-9197-4dbe5adbffb7","metadata":{"id":"7c6f68a4-2d83-46c6-9197-4dbe5adbffb7"},"source":["## Define SFT Trainer"]},{"cell_type":"code","execution_count":null,"id":"632a89cb-5e4d-48c1-b9c6-8d018bc5a32f","metadata":{"id":"632a89cb-5e4d-48c1-b9c6-8d018bc5a32f","outputId":"889671f3-672b-4c40-fe83-0d464f8e15c3","colab":{"referenced_widgets":["eb1097c92d544c7b98429420d9e0124c","6188e7b894d746a195698bb77912213d"]},"collapsed":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, packing, max_seq_length. Will not be supported from version '1.0.0'.\n","\n","Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:192: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb1097c92d544c7b98429420d9e0124c","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (2228 > 2048). Running this sequence through the model will result in indexing errors\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6188e7b894d746a195698bb77912213d","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","Using auto half precision backend\n"]}],"source":["from trl import SFTTrainer\n","from peft import LoraConfig\n","from transformers import TrainingArguments\n","\n","# path where the Trainer will save its checkpoints and logs\n","output_dir = './v3.0-RAFT-mistral-7b-lora-v2.0'\n","\n","# based on config\n","training_args = TrainingArguments(\n","    fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n","    do_eval=True,\n","    evaluation_strategy=\"epoch\",\n","    gradient_accumulation_steps=5,\n","    gradient_checkpointing=True,\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    learning_rate=1e-5, # Set the learning rate to a small value (0.00001) to ensure gradual and stable updates during fine-tuning\n","    log_level=\"info\", # Set the logging level to 'info' to capture general information during the training process\n","    logging_steps=10, # Log training information every 10 steps to monitor progress\n","    logging_strategy=\"steps\", # Log based on the number of steps rather than time intervals\n","    lr_scheduler_type=\"cosine\", # Use a cosine annealing schedule for the learning rate, which gradually reduces the learning rate following a cosine curve\n","    max_steps=-1, # Set the maximum number of training steps; -1 means no limit, and the number of steps is determined by the number of epochs\n","    num_train_epochs=10, # Set the total number of training epochs to 10, meaning the model will iterate over the entire dataset 10 times\n","    output_dir=output_dir,\n","    optim=\"paged_adamw_8bit\",\n","    overwrite_output_dir=True,\n","    per_device_eval_batch_size=1, # originally set to 8\n","    per_device_train_batch_size=1, # originally set to 8\n","    push_to_hub=True, #\n","    hub_model_id=\"v3.0-RAFT-mistral-7b-lora\", #\n","    hub_strategy=\"every_save\", #\n","    #report_to=\"tensorboard\",\n","    save_strategy=\"no\",\n","    save_total_limit=None,\n","    seed=42,\n",")\n","\n","# based on config\n","peft_config = LoraConfig(\n","        r=8,\n","        lora_alpha=16,\n","        lora_dropout=0.1,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",")\n","\n","trainer = SFTTrainer(\n","        model=model,\n","        #model_init_kwargs=model_kwargs,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        dataset_text_field=\"text\",\n","        tokenizer=tokenizer,\n","        packing=True,\n","        peft_config=peft_config,\n","        max_seq_length=tokenizer.model_max_length,\n","    )"]},{"cell_type":"code","execution_count":null,"id":"3d03e414-c0e4-4eeb-b2d4-1c5a2864bf97","metadata":{"colab":{"referenced_widgets":["e29c5830d8b64fc29d632fa8bc284978"]},"id":"3d03e414-c0e4-4eeb-b2d4-1c5a2864bf97","outputId":"67d1c7bf-4984-42b0-caa2-94cf5b7eab88"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 208\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 1\n","  Total train batch size (w. parallel, distributed & accumulation) = 5\n","  Gradient Accumulation steps = 5\n","  Total optimization steps = 410\n","  Number of trainable parameters = 6,815,744\n","Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmirkofedi2001\u001b[0m (\u001b[33mmirko5301\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/workspace/wandb/run-20240717_181027-x86fdxvw</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/mirko5301/huggingface/runs/x86fdxvw' target=\"_blank\">./v3.0-RAFT-mistral-7b-lora-v2.0</a></strong> to <a href='https://wandb.ai/mirko5301/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/mirko5301/huggingface' target=\"_blank\">https://wandb.ai/mirko5301/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/mirko5301/huggingface/runs/x86fdxvw' target=\"_blank\">https://wandb.ai/mirko5301/huggingface/runs/x86fdxvw</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [410/410 1:09:19, Epoch 9/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>1.586900</td>\n","      <td>1.600473</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>1.488400</td>\n","      <td>1.496033</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.401600</td>\n","      <td>1.412956</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.344900</td>\n","      <td>1.344537</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.267500</td>\n","      <td>1.241731</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>1.261700</td>\n","      <td>1.213062</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.200800</td>\n","      <td>1.198962</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.204200</td>\n","      <td>1.193846</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.207700</td>\n","      <td>1.193263</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","***** Running Evaluation *****\n","  Num examples = 208\n","  Batch size = 1\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=410, training_loss=1.349628962540045, metrics={'train_runtime': 4168.3223, 'train_samples_per_second': 0.499, 'train_steps_per_second': 0.098, 'total_flos': 1.79292063399936e+17, 'train_loss': 1.349628962540045, 'epoch': 9.85576923076923})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"127cbf29-b3ca-4264-aa92-b841eef0e74b","metadata":{"id":"127cbf29-b3ca-4264-aa92-b841eef0e74b"},"outputs":[],"source":["\n","trainer.save_state()"]},{"cell_type":"markdown","id":"e2194e10-674a-462f-b9bf-98e173a0d693","metadata":{"id":"e2194e10-674a-462f-b9bf-98e173a0d693"},"source":["## Evaluate model with prompt"]},{"cell_type":"code","execution_count":null,"id":"8a5dad17-92c7-4cd1-9f2b-161ac03b553c","metadata":{"id":"8a5dad17-92c7-4cd1-9f2b-161ac03b553c","outputId":"57bcd142-5e82-4ba4-f260-1854e01d2116"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<s><s>[INST] What are the resource allocations for the Chainlink network? here are the inputs During the initial coin offering (ICO) for LINK in September 2017, Chainlink announced a total and maximum supply of 1,000,000,000 LINK tokens. The current supply is about 453,509,553 LINK tokens, or about 45% of the total supply, as of end-September 2021. The Chainlink price at ICO was $0.11 and a total of 350 million LINK tokens were sold. This represents an over 200X from the ICO price to Chainlink price today.\n","\n","Chainlink price experienced a massive bull run in the period around mid-2019 to mid-2020. Chainlink bulls were colloquially referred to as “LINK Marines,'' becoming a well-known meme in the crypto community. Chainlink price reached an all-time high of $52.88 on May 9, 2021, on the back of an overall crypto market rally, as well as ongoing developments in the Chainlink ecosystem.\n","\n","According to the ICO documentation, 35% of the total token supply will go towards node operators and the incentivization of the ecosystem. Another 35% of LINK tokens were distributed during public sale events. Lastly, the remaining 30% of the total token supply was directed towards the company for the continued development of the Chainlink ecosystem and network. [/INST] Based on the information provided, here is a breakdown of the resource allocations for the Chainlink network:\n","\n","1. Node operators and ecosystem incentivization: 35% of the total token supply, which is equal to 350,000,000 LINK tokens.\n","2. Public sale events: 35% of the total token supply, which is equal to 350,000,000 LINK tokens.\n"]}],"source":["text = \"\"\"<s>[INST] What are the resource allocations for the Chainlink network? here are the inputs During the initial coin offering (ICO) for LINK in September 2017, Chainlink announced a total and maximum supply of 1,000,000,000 LINK tokens. The current supply is about 453,509,553 LINK tokens, or about 45% of the total supply, as of end-September 2021. The Chainlink price at ICO was $0.11 and a total of 350 million LINK tokens were sold. This represents an over 200X from the ICO price to Chainlink price today.\n","\n","Chainlink price experienced a massive bull run in the period around mid-2019 to mid-2020. Chainlink bulls were colloquially referred to as “LINK Marines,'' becoming a well-known meme in the crypto community. Chainlink price reached an all-time high of $52.88 on May 9, 2021, on the back of an overall crypto market rally, as well as ongoing developments in the Chainlink ecosystem.\n","\n","According to the ICO documentation, 35% of the total token supply will go towards node operators and the incentivization of the ecosystem. Another 35% of LINK tokens were distributed during public sale events. Lastly, the remaining 30% of the total token supply was directed towards the company for the continued development of the Chainlink ecosystem and network. [/INST]\"\"\"\n","\n","# Define the device for model inference\n","device = \"cuda:0\"\n","\n","# Tokenize the input text\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","# Generate output based on the input\n","outputs = model.generate(**inputs, max_new_tokens=100)\n","\n","# Decode and print the generated output\n","print(tokenizer.decode(outputs[0], skip_special_tokens=False))"]},{"cell_type":"code","execution_count":null,"id":"0867c54f-8e77-48e9-8c09-23efc30af459","metadata":{"id":"0867c54f-8e77-48e9-8c09-23efc30af459","outputId":"d6c79619-5c4f-4fcf-d27b-34b6121d2a0f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<s> What are the organisations and people involved in the development of Ripple?\n","\n","Ripple is an open-source, decentralized payment protocol that enables the transfer of various types of assets, including cryptocurrencies, fiat currencies, and other commodities. The Ripple protocol was initially developed by a company called OpenCoin, which was later renamed Ripple Labs.\n","\n","Ripple Labs was founded in 2012 by Chris Larsen and Jed McCaleb. Larsen served as the CEO\n"]}],"source":["text = \"\"\"What are the organisations and people involved in the development of Ripple?\"\"\"\n","\n","# Define the device for model inference\n","device = \"cuda:0\"\n","\n","# Tokenize the input text\n","inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","# Generate output based on the input\n","outputs = model.generate(**inputs, max_new_tokens=100)\n","\n","# Decode and print the generated output\n","print(tokenizer.decode(outputs[0], skip_special_tokens=False))"]},{"cell_type":"markdown","id":"2d02bc8d-bb98-4aa5-b767-1744be8af9b8","metadata":{"id":"2d02bc8d-bb98-4aa5-b767-1744be8af9b8"},"source":["## Saving model"]},{"cell_type":"code","execution_count":null,"id":"9ba95d3a-cc56-42c6-9a0d-14b2c5b2d1a0","metadata":{"colab":{"referenced_widgets":["eacd3e4072314cf7a4e7cf0938b83128","663e30aa3cba4f818feee8cbd53ef790","42482e81ff6c436c8f242958f0a67a67","3bcaae42a09049ea89d132086d0c1f7d","859435ebdb64444290ac4abd47ab235d","af54bf164afd4a2e946c86dacc2460e3"]},"id":"9ba95d3a-cc56-42c6-9a0d-14b2c5b2d1a0","outputId":"ef968503-ff44-49c0-caf2-b004ab8e8f07","collapsed":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to v3.0-RAFT-mistral-crypto\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/config.json\n","Model config MistralConfig {\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.43.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in v3.0-RAFT-mistral-crypto/tokenizer_config.json\n","Special tokens file saved in v3.0-RAFT-mistral-crypto/special_tokens_map.json\n","Saving model checkpoint to ./v3.0-RAFT-mistral-7b-lora-v2.0\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/config.json\n","Model config MistralConfig {\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.43.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","tokenizer config file saved in ./v3.0-RAFT-mistral-7b-lora-v2.0/tokenizer_config.json\n","Special tokens file saved in ./v3.0-RAFT-mistral-7b-lora-v2.0/special_tokens_map.json\n","Dropping the following result as it does not have all the necessary fields:\n","{'dataset': {'name': 'generator', 'type': 'generator', 'config': 'default', 'split': 'train', 'args': 'default'}}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bcaae42a09049ea89d132086d0c1f7d","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"859435ebdb64444290ac4abd47ab235d","version_major":2,"version_minor":0},"text/plain":["training_args.bin:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af54bf164afd4a2e946c86dacc2460e3","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["trainer.save_model(\"v3.0-RAFT-mistral-crypto\")"]},{"cell_type":"markdown","id":"011c6219-5a22-453b-86ee-7122426d3c73","metadata":{"id":"011c6219-5a22-453b-86ee-7122426d3c73"},"source":["## Testing huggingface model"]},{"cell_type":"code","execution_count":null,"id":"f54c25f1-8fe3-4ed1-bd11-74a0460148a4","metadata":{"colab":{"referenced_widgets":["44c52636f3d647949e9a6591a12be968","20fdbf60894c49b2a5ee105260e9bc46","21748567383744a397c093f1a602042e","c6c621acf4dc4b0fb20012822f91c332","52589a8d64554d848c250cd7ba8f68f3","373cc665b96349069c94df14a29cd3b7"]},"id":"f54c25f1-8fe3-4ed1-bd11-74a0460148a4","outputId":"ffa7037d-57de-420d-837f-85545edc51cc"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44c52636f3d647949e9a6591a12be968","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20fdbf60894c49b2a5ee105260e9bc46","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21748567383744a397c093f1a602042e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading file tokenizer.model from cache at None\n","loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mirko5301--mistral-7b-instruct-lora/snapshots/22df6a57498aec525465401ebe2429e4829bbf81/tokenizer.json\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mirko5301--mistral-7b-instruct-lora/snapshots/22df6a57498aec525465401ebe2429e4829bbf81/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mirko5301--mistral-7b-instruct-lora/snapshots/22df6a57498aec525465401ebe2429e4829bbf81/tokenizer_config.json\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6c621acf4dc4b0fb20012822f91c332","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/config.json\n","Model config MistralConfig {\n","  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n","  \"architectures\": [\n","    \"MistralForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 4096,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 14336,\n","  \"max_position_embeddings\": 32768,\n","  \"model_type\": \"mistral\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 8,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.43.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/model.safetensors.index.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52589a8d64554d848c250cd7ba8f68f3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["All model checkpoint weights were used when initializing MistralForCausalLM.\n","\n","All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n","loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/250544c9a802b0396550d0fd24bc80ff98bb1f5f/generation_config.json\n","Generate config GenerationConfig {\n","  \"bos_token_id\": 1,\n","  \"eos_token_id\": 2\n","}\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"373cc665b96349069c94df14a29cd3b7","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/218M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["What is Algorand? Algorand is an open-source, decentralized, and blockchain-based platform designed for building decentralized applications (dApps) and financial services. It was created by MIT professor Silvio Micali and his team\n"]}],"source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"mirko5301/mistral-7b-instruct-lora\")\n","model = AutoModelForCausalLM.from_pretrained(\"mirko5301/mistral-7b-instruct-lora\")\n","\n","text = \"\"\"What is Algorand?\"\"\"\n","\n","device = \"cuda:0\"\n","\n","inputs = tokenizer(text, return_tensors=\"pt\")\n","\n","\n","outputs = model.generate(**inputs, max_new_tokens=50)\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))"]},{"cell_type":"code","execution_count":null,"id":"8db884df-cba7-46af-abda-9a00ba71d271","metadata":{"id":"8db884df-cba7-46af-abda-9a00ba71d271"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}